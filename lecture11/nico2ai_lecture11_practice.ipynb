{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nico2ai_lecture11_practice.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"JyVRM7rEVSXY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!pip install gym\n","!pip install JSAnimation\n","!apt-get install python-opengl xvfb -y\n","!pip uninstall pyglet -y\n","!pip install pyglet==1.2.4\n","!pip install piglet\n","!pip install PyOpenGL\n","!pip install pyvirtualdisplay"],"execution_count":0,"outputs":[]},{"metadata":{"id":"colNFMmytHcO","colab_type":"text"},"cell_type":"markdown","source":["# NICO2AI Reinforcement Learning I Practice\n","\n","## Grid World with Q Learning\n","Grid Worldは2次元マトリックスで表された環境です．OpenAI GymでもFronzenLakeという環境がありますが，確率的に状態遷移する環境のため，本章ではGymと同じインターフェースをもつオリジナルの4x4マトリックスの環境を使います．この環境での目標は負の報酬が発生する穴を避けながらゴールを目指すことです．それではこの環境での強化学習設定をいかに示します．\n","\n","- 状態: グリッド中の位置を示す整数値(0~15)\n","- 報酬: 報酬を示す整数値(0/1/-1)\n","- 行動: 移動する方向を示す整数値(0~3)\n","- 価値: 状態行動テーブルに基づくQ値\n","\n","以下に環境のコードを示します．"]},{"metadata":{"id":"KSi_GwRCvqYo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","from IPython.display import display, clear_output\n","\n","class GridWorld:\n","    def __init__(self):\n","        # s: normal space\n","        # h: hole\n","        # g: goal\n","        self.grid = [\n","            ['s', 's', 's', 's'],\n","            ['s', 'h', 's', 'h'],\n","            ['s', 's', 's', 'h'],\n","            ['h', 's', 's', 'g']\n","        ]\n","        self.position = 0\n","    \n","    def reset(self):\n","        self.position = 0\n","        return 0\n","    \n","    def step(self, action):\n","        # right\n","        if action == 0 and (self.position + 1) % 4 != 0:\n","            self.position += 1\n","        # left\n","        elif action == 1 and self.position % 4 != 0:\n","            self.position -= 1\n","        # down\n","        elif action == 2 and self.position < 12:\n","            self.position += 4\n","        # up\n","        elif action == 3 and self.position > 4:\n","            self.position -= 4\n","        obj = self.grid[int(self.position / 4)][self.position % 4]\n","        if obj == 's':\n","            reward = 0\n","        elif obj == 'h':\n","            reward = -1\n","        elif obj == 'g':\n","            reward = 1\n","        done = obj == 'g' or obj == 'h'\n","        return self.position, reward, done, self.grid\n","\n","env = GridWorld()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AvNvrahGxW67","colab_type":"text"},"cell_type":"markdown","source":["上のコードを実行する事で前のエクササイズのバンディットと同様にGrid Worldの環境を作成できました．それではブランクを埋めてグリッドワールドのQ学習を行うエージェントを実装して見ましょう！"]},{"metadata":{"id":"_aQWS2I_xt6a","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class Agent:\n","    def __init__(self, num_states, num_actions, discount=0.99, lr=0.1, epsilon=0.3):\n","        self.num_states = num_states\n","        self.num_actions = num_actions\n","        self.discount = discount\n","        self.lr = lr\n","        self.epsilon = epsilon\n","        self.table = np.zeros((num_states, num_actions), dtype=np.float32)\n","    \n","    def act(self, state, greedy=False):\n","        # epsilon-greedy action selection\n","        # code here\n","    \n","    def learn(self, state, action, reward, next_state, done):\n","        q = self.table[state][action]\n","        # next_q should be zero when the end of episode\n","        next_q = # code here\n","        td_error = # code here\n","        self.table[state][action] += # code here\n","        return td_error\n","    \n","    def reset(self):\n","        self.table = np.zeros_like(self.table, dtype=np.float32)\n","    \n","    def render_table(self):\n","        plt.imshow(self.table, cmap=\"Greens\")\n","        ax = plt.gca()\n","        ax.grid(linewidth=0)\n","    \n","    def render_path(self):\n","        plt.imshow(np.max(self.table, axis=1).reshape(4, 4), cmap=\"Greens\")\n","        ax = plt.gca()\n","        ax.grid(linewidth=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JX8DCVgkyEZN","colab_type":"text"},"cell_type":"markdown","source":["実際に作ったエージェントで学習して見ましょう！ここでは学習の様子を面白くするために，行動を行う度に発生する報酬`step_reward`を導入して，この値を変化させるとどのように学習が変化するかを観察して見ましょう．"]},{"metadata":{"id":"W44EBjG8xRRF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# change these values later\n","discount = 0.9\n","lr = 0.1\n","epsilon = 0.3\n","step_reward = -0.01\n","\n","agent = Agent(16, 4, discount, lr, epsilon)\n","\n","rewards = []\n","for i in range(1000):\n","    state = env.reset()\n","    last_state = None\n","    sum_of_reward = 0\n","    while True:\n","        last_state = state\n","        action = agent.act(state)\n","        state, reward, done, _ = env.step(action)\n","        reward += step_reward\n","        agent.learn(last_state, action, reward, state, done)\n","        sum_of_reward += reward\n","        if done:\n","            break\n","    rewards.append(sum_of_reward)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A3f3ifOz3spk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# plot q table\n","agent.render_table()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-v8PAYgFy2Mo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# plot max action values\n","agent.render_path()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"moA43QddzWAh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# plot moving average of rewards\n","plt.plot(np.arange(1000), np.convolve(rewards, np.ones(10) / 10, mode='same'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yxZ5bPcyNW69","colab_type":"text"},"cell_type":"markdown","source":["予想通りの結果が得られましたか？もしそうなら割引率や学習率，ε，`step_reward`の値を変えて実験して見ましょう．"]},{"metadata":{"id":"XTN4RZcxNrrC","colab_type":"text"},"cell_type":"markdown","source":["## CartPole with Q Learning\n","CartPoleは強化学習では有名でカートを動かしてポールを垂直に立たせるのを目標とするタスクです．最初に以下のコードを実行してアニメーションを見て見ましょう．"]},{"metadata":{"id":"di1gAc5XPNtD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import gym\n","from JSAnimation.IPython_display import display_animation\n","from matplotlib import animation\n","from pyvirtualdisplay import Display\n","\n","# virtual display settings to render gym\n","v_display = Display(visible=0, size=(1400, 900))\n","v_display.start()\n","\n","## util function to render CartPole as GIF image\n","def display_frames_as_gif(frames):\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","\n","    def animate(i):\n","        patch.set_data(frames[i])\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n","    display(display_animation(anim, default_mode='loop'))\n","\n","env = gym.make('CartPole-v0')\n","env.reset()\n","frames = []\n","while True:\n","    frames.append(env.render(mode='rgb_array'))\n","    _, _, done, _ = env.step(0)\n","    if done:\n","        break\n","display_frames_as_gif(frames)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OVIp2qjNe6x6","colab_type":"text"},"cell_type":"markdown","source":["CartPoleはOpenAI Gymからも提供されています．強化学習エージェントはカートを右か左に動かします．ポールが上を向いている間，毎時刻で+1の報酬が得られます．ポールが15度以上傾くかカートが2.4（単位距離）以上移動するとエピソードが終了します．この環境の強化学習設定は以下のようになります．\n","\n","- 状態: 4次元配列 (水平位置，水平速度，ポールの角度，ポールの角速度)\n","- 行動: カートを動かす方向を表す整数値(0/1)\n","- 報酬: 毎時刻+1\n","- 価値: テーブルで表されたQ値\n","\n","GridWorldとCartPoleの違いは状態が連続値である点です．状態が連続値の場合はテーブルで扱うことができないため，状態空間を離散化する必要があります．本章では`np.linspace(start, stop, num)`と`np.digitize(x, bins)`を用いて値の離散化を行います．`np.linspace(start, stop, num)`は`start`と`end`の区間を`num`個に分割した配列を返します．`np.digitize(x, bins)`は`bins[index - 1] =< x < bins[index]`を満たす`index`を返します．それでは状態の離散化を行って見ましょう．"]},{"metadata":{"id":"CULfE8GVcQIF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# change these values later\n","horizontal_position_num = 2\n","horizontal_velocity_num = 10\n","pole_angle_num = 50\n","pole_angular_velocity_num = 20\n","\n","horizontal_position_bins = np.linspace(-2.4, 2.4, horizontal_position_num)\n","horizontal_velocity_bins = np.linspace(-2.0, 2.0, horizontal_velocity_num)\n","pole_angle_bins = np.linspace(-0.4, 0.4, pole_angle_num)\n","pole_angular_velocity_bins = np.linspace(-3.5, 3.5, pole_angular_velocity_num)\n","\n","# return index of flatten 4 dimensional matrix\n","def discretize(state):\n","    # code here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UBiR2A2jrvkA","colab_type":"text"},"cell_type":"markdown","source":["これで，4次元の連続値を`2x10x50x20`の1次元配列のindexとして返す関数が実装できました．それでは先ほど作った強化学習エージェントで学習して見ましょう．"]},{"metadata":{"id":"Ubh2CP6FsGd9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# change these values later\n","discount = 0.9\n","lr = 0.1\n","epsilon = 0.1\n","\n","num_state = horizontal_position_num *\\\n","        horizontal_velocity_num * pole_angle_num * pole_angular_velocity_num\n","\n","agent = Agent(num_state, 2, discount, lr, epsilon)\n","\n","rewards = []\n","for i in range(10000):\n","    state = env.reset()\n","    last_state = None\n","    sum_of_rewards = 0\n","    \n","    while True:\n","        last_state = state\n","        action = agent.act(discretize(state))\n","        state, reward, done, _ = env.step(action)\n","        agent.learn(discretize(last_state), action, reward, discretize(state), done)\n","        sum_of_rewards += reward\n","        if done:\n","            break\n","            \n","    rewards.append(sum_of_rewards)\n","\n","# this may take a few minutes\n","plt.plot(np.arange(10000), np.convolve(rewards, np.ones(100) / 100, mode='same'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Yx0Nst53snr9","colab_type":"text"},"cell_type":"markdown","source":["エージェントが正しく学習されて入れば，スコアが上がっているのが確認できます．最後にエージェントの振る舞いを確認して見ましょう．"]},{"metadata":{"id":"svzw2F4stDhb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# evaluate trained agent\n","state = env.reset()\n","frames = []\n","t = 0\n","while True:\n","    frames.append(env.render(mode='rgb_array'))\n","    action = agent.act(discretize(state), greedy=True)\n","    state, _, done, _ = env.step(action)\n","    t += 1\n","    if done:\n","        break\n","env.render()\n","display_frames_as_gif(frames)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qOjIJowAtS7L","colab_type":"text"},"cell_type":"markdown","source":["おめでとうございます！これでQ学習をマスターできました！"]}]}