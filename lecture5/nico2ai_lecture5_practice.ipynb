{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NICO2AI 第5回 勾配法と誤差逆伝搬法 (7/15) 実践演習\n",
    "\n",
    "課題\n",
    "\n",
    "- 3層NNモデルのクロスエントロピー誤差最小化をミニバッチ勾配降下法で実装する\n",
    "- MNISTデータセットを用いて学習を行う\n",
    "\n",
    "`### CODE HERE ###` と記載されている部分にコードを埋めていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget \"https://drive.google.com/uc?export=download&id=1FfK_OGcOU5Jy_jhkXlPYhoq0LmJBIiDB\" -O utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.random.seed(111)\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from utils import to_categorical, calculate_accuracy, plot_confusion_matrix, get_image_tile\n",
    "\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNISTデータのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original', data_home='/tmp')\n",
    "np.random.seed(111)\n",
    "data_idx = np.arange(70000)\n",
    "np.random.shuffle(data_idx)\n",
    "X_train = mnist['data'][data_idx][:50000]\n",
    "X_valid = mnist['data'][data_idx][50000:60000]\n",
    "X_test = mnist['data'][data_idx][60000:]\n",
    "y_train = mnist['target'][data_idx][:50000]\n",
    "y_valid = mnist['target'][data_idx][50000:60000]\n",
    "y_test = mnist['target'][data_idx][60000:]\n",
    "\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_valid = to_categorical(y_valid)\n",
    "Y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = get_image_tile(X_train, width=10, height=10)\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各種関数の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(U, reduce_axis=0):\n",
    "  shp = list(U.shape)\n",
    "  shp[reduce_axis] = 1\n",
    "  return ### CODE HERE ###\n",
    "\n",
    "def softmax_cross_entropy(D, Y):\n",
    "  epsilon = 1e-8\n",
    "  Y = np.clip(Y, epsilon, 1-epsilon)\n",
    "  return ### CODE HERE ###\n",
    "\n",
    "def sigmoid(U):\n",
    "  return ### CODE HERE ###\n",
    "\n",
    "def dsigmoid_du(U):\n",
    "  return ### CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## エラーチェック\n",
    "\n",
    "DIM = 20\n",
    "NB_CLASS = 10\n",
    "N = 100\n",
    "U = np.random.normal(size=DIM*N).reshape(DIM, N)\n",
    "D = np.zeros([NB_CLASS, N])\n",
    "D[4] = 1.0\n",
    "Y = np.zeros([NB_CLASS, N])\n",
    "Y[:] = 1.0 / NB_CLASS\n",
    "assert softmax(U).shape == (DIM, N), 'softmax(U).shape must be {}. result: {}'.format((DIM, N), softmax(U).shape)\n",
    "assert softmax_cross_entropy(D, Y).shape == (), \\\n",
    "  'softmax_cross_entropy(D, Y).shape must be {}. result: {}'.format((), softmax_cross_entropy(D, Y).shape)\n",
    "assert 220 <= softmax_cross_entropy(D, Y) <= 240, \\\n",
    "  'softmax_cross_entropy(D, Y) must approximately equal to 230, when the values of all elements of Y are equal.'\n",
    "assert sigmoid(U).shape == (DIM, N), 'sigmoid(U).shape must be {}. result: {}'.format((DIM, N), sigmoid(U).shape)\n",
    "assert dsigmoid_du(U).shape == (DIM, N), \\\n",
    "  'dsigmoid_du(U).shape must be {}. result: {}'.format((DIM, N), dsigmoid_du(U).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメーター初期化関数の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_dim = 784\n",
    "h_dim = 256\n",
    "nb_classes = 10\n",
    "\n",
    "def init_params():\n",
    "  W_2 = np.random.normal(loc=0.0, scale=1.0, size=x_dim*h_dim).reshape([h_dim, x_dim])\n",
    "  b_2 = np.zeros(h_dim).reshape(h_dim, 1)\n",
    "  W_3 = np.random.normal(loc=0.0, scale=1.0, size=h_dim*nb_classes).reshape([nb_classes, h_dim])\n",
    "  b_3 = np.zeros(nb_classes).reshape(nb_classes, 1)\n",
    "  return W_2, b_2, W_3, b_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論用関数の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(X):\n",
    "  X = X.T\n",
    "  assert X.shape[0] == x_dim\n",
    "\n",
    "  U_2 = ### CODE HERE ###\n",
    "  Z_2 = ### CODE HERE ###\n",
    "\n",
    "  U_3 = ### CODE HERE ###\n",
    "  Z_3 = ### CODE HERE ###\n",
    "\n",
    "  return Z_3.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## エラーチェック\n",
    "\n",
    "W_2, b_2, W_3, b_3 = init_params()\n",
    "assert inference(X_train[:100]).shape == (100, NB_CLASS), \\\n",
    "  'inference(X).shape must be {}. result: {}'.format((100, NB_CLASS), inference(X_train[:100]).shape)\n",
    "assert inference(X_train[:100]).sum(axis=1).shape == np.ones(100).shape, \\\n",
    "  'The sum around the class of output of inference(X) must be 1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 10\n",
    "batch_size = 100\n",
    "nb_batch = int(len(X_train) / batch_size)\n",
    "eta = 0.01\n",
    "data_idx = np.arange(len(X_train))\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "ax = fig.add_subplot(111)\n",
    "train_epochs = []\n",
    "valid_epochs = []\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "plot_freq = 50\n",
    "\n",
    "W_2, b_2, W_3, b_3 = init_params()\n",
    "\n",
    "for epoch in range(nb_epoch):\n",
    "  np.random.shuffle(data_idx)\n",
    "  for batch in range(nb_batch):\n",
    "    X_batch = X_train[data_idx[batch*batch_size:(batch+1)*batch_size]]\n",
    "    Y_batch = Y_train[data_idx[batch*batch_size:(batch+1)*batch_size]]\n",
    "    X = X_batch.T\n",
    "    D = Y_batch.T\n",
    "\n",
    "    ## 順方向の推論\n",
    "    U_2 = ### CODE HERE ###\n",
    "    Z_2 = ### CODE HERE ###\n",
    "\n",
    "    U_3 = ### CODE HERE ###\n",
    "    Z_3 = ### CODE HERE ###\n",
    "    Y = Z_3\n",
    "\n",
    "    ## デルタの計算\n",
    "    Delta_3 = ### CODE HERE ###\n",
    "    Delta_2 = ### CODE HERE ###\n",
    "\n",
    "    ## 3層目のパラメーターに関する勾配\n",
    "    dLdW_3 = ### CODE HERE ###\n",
    "    dLdb_3 = ### CODE HERE ###\n",
    "\n",
    "    ## 2層目のパラメーターに関する勾配\n",
    "    dLdW_2 = ### CODE HERE ###\n",
    "    dLdb_2 = ### CODE HERE ###\n",
    "\n",
    "    ## 3層目のパラメーターの更新\n",
    "    W_3 = ### CODE HERE ###\n",
    "    b_3 = ### CODE HERE ###\n",
    "\n",
    "    ## 2層目のパラメーターの更新\n",
    "    W_2 = ### CODE HERE ###\n",
    "    b_2 = ### CODE HERE ###\n",
    "\n",
    "    ## リアルタイムの誤差の描画\n",
    "    if batch % plot_freq == 0:\n",
    "      train_epochs.append( epoch+batch/nb_batch )\n",
    "      train_losses.append( softmax_cross_entropy(D, Y) / batch_size )\n",
    "      train_accuracies.append( calculate_accuracy(D.argmax(axis=0), Y.argmax(axis=0)) )\n",
    "      clear_output(wait = True)\n",
    "      ax.plot( train_epochs, train_losses, label='Train' )\n",
    "      ax.plot( valid_epochs, valid_losses, label='Validation' )\n",
    "      plt.xlabel('Epoch')\n",
    "      plt.ylabel('Loss')\n",
    "      plt.title('epoch: {:02d}, batch: {:04d}'.format(epoch, batch))\n",
    "      plt.legend()\n",
    "      display(fig)\n",
    "      ax.cla()\n",
    "  Y = inference(X_valid).T\n",
    "  valid_epochs.append( epoch+1 )\n",
    "  valid_losses.append( softmax_cross_entropy(Y_valid.T, Y) / len(Y_valid) )\n",
    "  valid_accuracies.append( calculate_accuracy(y_valid, Y.argmax(axis=0)) )\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot( train_epochs, train_accuracies, label='Train' )\n",
    "ax.plot( valid_epochs, valid_accuracies, label='Validation' )\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習結果の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_hat = inference(X_valid)\n",
    "y_hat = Y_hat.argmax(axis=1)\n",
    "valacc = calculate_accuracy(y_valid, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = confusion_matrix(y_valid, y_hat)\n",
    "plot_confusion_matrix(C, range(10))\n",
    "print('Validation accuracy: {:.4f}'.format(valacc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## テストデータでの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_hat = ### CODE HERE ###\n",
    "y_hat = Y_hat.argmax(axis=1)\n",
    "testacc = ### CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = confusion_matrix(y_test, y_hat)\n",
    "plot_confusion_matrix(C, range(10))\n",
    "print('Test accuracy: {:.4f}'.format(testacc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
