{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nico2ai_lecture12_practice.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"-XOSMnQozVF2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!pip install chainer\n","!pip install gym[atari]\n","!pip install JSAnimation\n","!apt-get install python-opengl xvfb cmake -y\n","!pip uninstall pyglet -y\n","!pip install pyglet==1.2.4\n","!pip install piglet\n","!pip install PyOpenGL\n","!pip install pyvirtualdisplay\n","!pip install chainerrl\n","!pip install opencv-python\n","!wget https://www.dropbox.com/s/z2sh6j8s8vr6qrv/dqn-trained-model.zip\n","!unzip dqn-trained-model.zip"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Eplo3r-yuInq","colab_type":"text"},"cell_type":"markdown","source":["# NICO2AI Lecture12 Reinforcement Learning II Practice\n","## ChainerRL\n","- website https://github.com/chainer/chainerrl\n","\n","ChainerRLはPFNが公開している強化学習用のライブラリです．ChainerRLには多くのアルゴリズムが実装されていて，Arcade Learning EnvironmentやOpenAI Gymで簡単に試すことができます．以下が実装されているアルゴリズムです．\n","\n","- DQN\n","- Double DQN\n","- Dueling DQN\n","- DDPG\n","- A3C\n","- N-step Q Learning\n","- PCL"]},{"metadata":{"id":"NBcBaiymzdml","colab_type":"text"},"cell_type":"markdown","source":["## CartPole with Deep Neural Network\n","ChainerRLの使い方を学ぶために，全結合層を用いたDQNを実装して見ましょう．前回取り組んだCartPoleの時とは違い，DQNはディープニューラルネットワークを用いることで連続的な入力を扱うことができます．入力から出力まで一貫して学習ができるため，まさにend-to-endという訳です．ではブランクを埋めてChainerRLを用いた実装を行いましょう．"]},{"metadata":{"id":"BzuMID4f2F4e","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","import chainer\n","import chainer.functions as F\n","import chainer.links as L\n","import chainerrl\n","import gym\n","from collections import deque\n","from IPython.display import display, clear_output\n","\n","env = gym.make('CartPole-v0')\n","\n","# deep neural network with 3 fully connected layers\n","class QFunction(chainer.Chain):\n","    def __init__(self, num_state, num_actions, n_hidden_channels=50):\n","        super().__init__(\n","            l0=L.Linear(num_state, n_hidden_channels),\n","            l1=L.Linear(n_hidden_channels, n_hidden_channels),\n","            l2=L.Linear(n_hidden_channels, num_actions))\n","\n","    def __call__(self, x):\n","        h = F.relu(self.l0(x))\n","        h = F.relu(self.l1(h))\n","        out = self.l2(h)\n","        # wrapper for discrete action space\n","        return chainerrl.action_value.DiscreteActionValue(out)\n","\n","# ChainerRL also has replay memory used in DQN, but create original one here.\n","class ReplayBuffer:\n","    def __init__(self, capacity):\n","        self.memory = deque(maxlen=capacity)\n","    \n","    def append(self, state, action, reward, next_state, next_action, is_state_terminal):\n","        experience = dict(state=state, action=action, reward=reward,\n","                next_state=next_state, next_action=next_action,\n","                is_state_terminal=is_state_terminal)\n","        self.memory.append(experience)\n","    \n","    def sample(self, n):\n","        return random.sample(self.memory, n)\n","    \n","    def __len__(self):\n","        return len(self.memory)\n","    \n","    def stop_current_episode(self):\n","        pass\n","    \n","q_func = QFunction(4, 2)\n","\n","optimizer = chainer.optimizers.Adam(eps=1e-2)\n","optimizer.setup(q_func)\n","\n","# ChainerRL provides some exploration strategies\n","explorer = chainerrl.explorers.ConstantEpsilonGreedy(\n","    epsilon=0.1, random_action_func=lambda: np.random.choice(2))\n","\n","# Buffer size is 10^5 which is smaller than 10^6 used in the paper\n","# because replay buffer actually occupies large memory space\n","# replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 5)\n","replay_buffer = ReplayBuffer(capacity=10 ** 5)\n","\n","discount = 0.9\n","\n","# DQN agent offered by ChainerRL with modular structure\n","agent = chainerrl.agents.DQN(\n","    q_func, optimizer, replay_buffer, discount, explorer,\n","    replay_start_size=500, update_interval=1,\n","    target_update_interval=100, phi=lambda x: np.array(x, dtype=np.float32))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"G-NrHU6O4Y7E","colab_type":"text"},"cell_type":"markdown","source":["これでDQNエージェントが出来上がりました．では，学習させて見ましょう．"]},{"metadata":{"id":"Wi6PioZM4fH9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["rewards = []\n","# this may take a few minutes\n","for i in range(300):\n","    state = env.reset()\n","    sum_of_rewards = 0\n","    reward = 0\n","    \n","    while True:\n","        action = agent.act_and_train(state, reward)\n","        state, reward, done, _ = env.step(action)\n","        sum_of_rewards += reward\n","        if done:\n","            agent.stop_episode_and_train(state, reward, done)\n","            break\n","    rewards.append(sum_of_rewards)\n","\n","plt.plot(np.arange(300), np.convolve(rewards, np.ones(10) / 10, mode='same'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"stb-2df8A8nf","colab_type":"text"},"cell_type":"markdown","source":["それでは前回と同様にエージェントの振る舞いを見て見ましょう．"]},{"metadata":{"id":"sJu5orCWBF7n","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from JSAnimation.IPython_display import display_animation\n","from matplotlib import animation\n","from pyvirtualdisplay import Display\n","\n","# virtual display settings to render gym\n","v_display = Display(visible=0, size=(1400, 900))\n","v_display.start()\n","\n","## util function to render CartPole as GIF image\n","def display_frames_as_gif(frames):\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","\n","    def animate(i):\n","        patch.set_data(frames[i])\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n","    display(display_animation(anim, default_mode='loop'))\n","\n","# evaluate trained agent\n","state = env.reset()\n","frames = []\n","t = 0\n","while True:\n","    frames.append(env.render(mode='rgb_array'))\n","    action = agent.act(state)\n","    state, _, done, _ = env.step(action)\n","    t += 1\n","    if done:\n","        break\n","display_frames_as_gif(frames)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"W2uOzxSBHqYq","colab_type":"text"},"cell_type":"markdown","source":["## DQN on Atari\n","Atariのゲームを学習するには非常に長い時間が必要です（たとえGPUマシンを使用しても）．本章ではPongで学習したモデルを使用して振る舞いを確認して見ましょう．"]},{"metadata":{"id":"DS4EqLcVGbh0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import os\n","import cv2\n","from chainerrl import links\n","\n","\n","env = gym.make('PongDeterministic-v4')\n","\n","# ChainerRL provides some predefined networks\n","# in this case, q_func has 3 convolutional layers and 2 fully connected layers\n","# n_input_channels represents the number of images to feed networks\n","convs = links.NatureDQNHead(n_input_channels=4)\n","q_func = links.Sequence(convs, L.Linear(512, 6),\n","        chainerrl.action_value.DiscreteActionValue)\n","\n","# create optimizer to avoid errors when loading trained models\n","# but this RMSprop optimizer is actually same as the one in the paper\n","optimizer = chainer.optimizers.RMSpropGraves(\n","        lr=2.5e-4, alpha=0.95, momentum=0.0, eps=1e-2)\n","optimizer.setup(q_func)\n","\n","# most of arguments are None because agent would not train here\n","agent = chainerrl.agents.DQN(\n","    q_func, optimizer, None, None, None,\n","    phi=lambda x: np.array(x, dtype=np.float32) / 255.0)\n","\n","# load a trained model\n","agent.load(os.path.join(os.getcwd(), 'trained'))\n","\n","# deque object to hold 4 frames for frame skipping\n","states = deque(np.zeros((4, 84, 84)).tolist(), maxlen=4)\n","\n","# evaluate trained agent\n","state = env.reset()\n","frames = []\n","t = 0\n","while True:\n","    # process input images\n","    state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n","    state = cv2.resize(state, (84, 84))\n","    states.append(state)\n","    \n","    frames.append(env.render(mode='rgb_array'))\n","    action = agent.act(list(reversed(states)))\n","    state, _, done, _ = env.step(action)\n","    t += 1\n","    if done or t > 1000:\n","        break\n","display_frames_as_gif(frames)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kWOntEV2INBh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def visualize_conv_layer(weight, shape=(8, 8)):\n","    fig = plt.figure()\n","    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n","    for i in range(weight.shape[0]):\n","        ax = fig.add_subplot(shape[0], shape[1], i + 1, xticks=[], yticks=[])\n","        ax.imshow(weight[i][0], cmap=plt.cm.gray_r, interpolation='nearest')\n","    plt.show()\n","\n","# visualize the first convolutional layer\n","visualize_conv_layer(convs[0].W.data, (4, 8))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hAagxc59IQVk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# visualize the second convolutional layer\n","visualize_conv_layer(convs[1].W.data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2N_3Ex3RISHG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# visualize the third convolutional layer\n","visualize_conv_layer(convs[2].W.data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FHuIyilsIU37","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def visualize_activations(activations, shape=(8, 8)):\n","    fig = plt.figure()\n","    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n","    for i in range(activations.shape[0]):\n","        ax = fig.add_subplot(shape[0], shape[1], i + 1, xticks=[], yticks=[])\n","        ax.imshow(activations[i], cmap=plt.cm.gray_r, interpolation='nearest')\n","    plt.show()\n","\n","states = deque(np.zeros((4, 84, 84)).tolist(), maxlen=4)\n","env.reset()\n","for i in range(15):\n","    state, _, _, _ = env.step(0)\n","    state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n","    state = cv2.resize(state, (84, 84))\n","    states.append(state)\n","\n","# visualize activations on the first layer\n","h = F.relu(convs[0](np.array([list(reversed(states))], dtype=np.float32) / 255.0))\n","visualize_activations(h.data[0], (4, 8))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pxxOfuEyIYpp","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# visualize activations on the second layer\n","h = F.relu(convs[1](h))\n","visualize_activations(h.data[0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E34e2t2BIa8e","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# visualize activations on the third layer\n","h = F.relu(convs[2](h))\n","visualize_activations(h.data[0])"],"execution_count":0,"outputs":[]}]}