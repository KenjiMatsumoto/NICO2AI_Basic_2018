{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lecture7_practice_master.ipynb のコピー","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"bVh_7b64Z5TO","colab_type":"text"},"cell_type":"markdown","source":["## Colaboratory用\n","ランタイムをGPUにする\n","  - ランタイム -> ランタイムの変更\n","  - 「ハードウェアアクセラレータ」でGPUを選択"]},{"metadata":{"id":"QLhhC5TNZ5TQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# 必要なパッケージのインストール\n","\n","!apt -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1\n","!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n","!pip install https://github.com/kmaehashi/chainer-colab/releases/download/2018-02-06/cupy_cuda80-4.0.0b3-cp36-cp36m-linux_x86_64.whl\n","!pip install 'chainer==4.0.0b3'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vQOtnayXZ5TV","colab_type":"text"},"cell_type":"markdown","source":["## Import\n","お決まりのimprot"]},{"metadata":{"id":"waXEg2J2Z5TW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import sys\n","import numpy as np\n","import chainer\n","from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n","from chainer import datasets, iterators, optimizers, serializers\n","from chainer import Link, Chain, ChainList\n","import chainer.functions as F\n","import chainer.links as L\n","from chainer.training import extensions"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0ODQusNiZ5TY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import random"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_4YA7fAkZ5Tb","colab_type":"text"},"cell_type":"markdown","source":["# CIFAR-10"]},{"metadata":{"id":"AMA8U6dAZ5Tc","colab_type":"text"},"cell_type":"markdown","source":["## データセットのダウンロード"]},{"metadata":{"id":"cbCKKWFgZ5Td","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["train_and_val, test = datasets.get_cifar10(ndim=3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q6kK4J1KZ5Tf","colab_type":"text"},"cell_type":"markdown","source":["データの中身を見てみる"]},{"metadata":{"id":"-FtMangPZ5Tg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["cifar10_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ydEfBRTCZ5Ti","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","sample = train_and_val[0]\n","image = sample[0]\n","label = sample[1]\n","\n","plt.imshow(image.transpose((1, 2, 0)))\n","print('Label:', cifar10_labels[label])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EeCJVzaqZ5Tl","colab_type":"text"},"cell_type":"markdown","source":["## 【小課題】 DataAugmentationの実装\n","CIFAR-10のデータを対象に、\n","\n","- Horizontal flipping\n","- Random cropping\n","\n","を実装する"]},{"metadata":{"id":"AtqClFlpZ5Tl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def horizontal_flipping(img):\n","    ### Implement here!\n","\n","    ###\n","    return flipped_img"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_4Cr3YagZ5To","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Preview\n","img = train_and_val[0][0]\n","img = horizontal_flipping(img)\n","plt.imshow(img.transpose((1, 2, 0)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SG62HWnCZ5Ts","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def random_cropping(img, crop_size):\n","    ### Implement here!\n","\n","    ###\n","    return cropped_img"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-oqRcn2MZ5Tu","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Preview\n","img = train_and_val[0][0]\n","img = random_cropping(img, 24)\n","plt.imshow(img.transpose((1, 2, 0)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ziUIIAtWZ5Tw","colab_type":"text"},"cell_type":"markdown","source":["# CNNでImage classification"]},{"metadata":{"id":"54MINiFCZ5Tx","colab_type":"text"},"cell_type":"markdown","source":["## モデル定義"]},{"metadata":{"id":"BHrBkYEHZ5Ty","colab_type":"text"},"cell_type":"markdown","source":["例: LeNet5\n","https://docs.chainer.org/en/stable/tutorial/convnet.html#lenet5"]},{"metadata":{"id":"FpbNc_CZZ5Ty","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class LeNet5(Chain):\n","    def __init__(self, in_channels):\n","        super(LeNet5, self).__init__()\n","        with self.init_scope():\n","            self.conv1 = L.Convolution2D(\n","                in_channels=in_channels, out_channels=6, ksize=5, stride=1)\n","            self.conv2 = L.Convolution2D(\n","                in_channels=6, out_channels=16, ksize=5, stride=1)\n","            self.conv3 = L.Convolution2D(\n","                in_channels=16, out_channels=120, ksize=4, stride=1)\n","            self.fc4 = L.Linear(None, 84)\n","            self.fc5 = L.Linear(84, 10)\n","\n","    def __call__(self, x):\n","        h = F.sigmoid(self.conv1(x))\n","        h = F.max_pooling_2d(h, 2, 1)\n","        h = F.sigmoid(self.conv2(h))\n","        h = F.max_pooling_2d(h, 2, 1)\n","        h = F.sigmoid(self.conv3(h))\n","        h = F.sigmoid(self.fc4(h))\n","        if chainer.config.train:\n","            return self.fc5(h)\n","        return F.softmax(self.fc5(h))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E7k7-m-4Z5T1","colab_type":"text"},"cell_type":"markdown","source":["### 【課題】自分でCNNモデルを定義してみる\n","例：\n","```\n","Convolution (out_channels=32, k=5, s=1, p=0)\n","ReLU\n","BatchNormalization\n","MaxPooling (k=2, s=1)\n","Convolution (out_channels=64, k=3, s=1, p=0)\n","ReLU\n","BatchNormalization\n","Convolution (out_channels=64, k=3, s=1, p=0)\n","ReLU\n","BatchNormalization\n","MaxPooling (k=2, s=1)\n","FullyConnected (out_channels=512)\n","ReLU\n","FullyConnected (out_channels=10)\n","```\n","\n","##### 参考: 比較的よく採用される構成\n","- ReLU + Batch Normalization\n","  - どちらが先か、など試行錯誤も\n","- FC層は少なめ。FCはゼロで最後はGlobal Average Poolingにする構成も。"]},{"metadata":{"id":"dNpqR03WZ5T1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class MyModel(Chain):\n","    ### Implement here!\n","\n","    ###"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xdy8fGO14s5Q","colab_type":"text"},"cell_type":"markdown","source":["### データセットを取得"]},{"metadata":{"id":"pbXf5o6SZ5T6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# train_and_valをtrainとvalidationに分割\n","from chainer.datasets import split_dataset_random\n","\n","n_train = 40000\n","train, validation = split_dataset_random(train_and_val, n_train, seed=42)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UGIcNgHtZ5T3","colab_type":"text"},"cell_type":"markdown","source":["### 【課題】 DataAugmentationを行うDatasetを実装する\n","\n","`chainer.datasets.get_cifar10` からはCIFAR10のデータをwrapしたデータセットオブジェクトが得られます (`chainer.datasets.tuple_dataset.TupleDataset`)。\n","\n","```python\n","train_and_val, test = datasets.get_cifar10(ndim=3)\n","```\n","\n","この`train_and_val`や`test`は、そのままイテレータオブジェクト(`iterators.SerialIterator`など)に渡すことができます。\n","\n","```python\n","test_iter = iterators.SerialIterator(test, batch_size=100, shuffle=False, repeat=False)\n","```\n","\n","CIFAR10をそのまま使う場合はこれで良いのですが、\n","CIFAR10のデータに何らかの前処理を行ったりする場合は、独自にデータセットオブジェクト（データセットクラス）を作ることができます。\n","\n","`chainer.dataset.DatasetMixin` を継承したクラスを作り、`get_example(self, i)` メソッドを実装します。\n","\n","例:\n","```python\n","class MyDataset(chainer.dataset.DatasetMixin):\n","    def get_example(self, i):\n","        image = # get image data\n","        label = # get label data\n","        return image, label\n","```\n","\n","data augmentation（とmean subtraction）を行う独自データセットクラス`PreprocessedDataset`を実装してみましょう。"]},{"metadata":{"id":"9rHXSazN2r8X","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# chainer.datasetが扱う画像データは、sklearnとチャネルが違うので注意\n","# sklearn: (y, x, color)\n","# chainer: (color, y, x)\n","img, label = train.get_example(0)\n","print('Shape: ', img.shape)\n","\n","plt.imshow(img.transpose(1, 2, 0))  # (color, y, x) を (y, x, color) に変換して表示"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t7jRuyN0Z5T4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class PreprocessedDataset(chainer.dataset.DatasetMixin):\n","\n","    def __init__(self, base, mean=None, crop_size=30, random=True):\n","        if mean is None:\n","            mean = np.zeros((3, 1, 1))\n","        \n","        self.base = base\n","        self.mean = mean.astype('f').reshape((3, 1, 1))\n","        self.crop_size = crop_size\n","        self.random = random\n","\n","    def __len__(self):\n","        return len(self.base)\n","\n","    def get_example(self, i):\n","        image, label = self.base[i]\n","\n","        ### Implement here!\n","        # If random == True, apply the following processings to a given image\n","        # 1. Random cropping\n","        # 2. Random horizontal flipping\n","\n","        ###\n","        \n","        image -= self.mean # Subtract mean\n","        return image, label"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cNcTKGC8Z5T5","colab_type":"text"},"cell_type":"markdown","source":["## 学習"]},{"metadata":{"id":"HVOdCEEWZ5T8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Data augmentationを適用\n","train = PreprocessedDataset(train)\n","validation = PreprocessedDataset(validation, random=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LKhNBX8wZ5T-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["train_iter = iterators.SerialIterator(train, batch_size=100, shuffle=True)\n","validation_iter = iterators.SerialIterator(validation, batch_size=100, repeat=False, shuffle=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4OrWGJk1Z5T_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["device = 0  # GPU:0, CPU: -1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AXebvMw8Z5UB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["net = MyModel()\n","model = L.Classifier(net)\n","\n","optimizer = optimizers.Adam()\n","optimizer.setup(model)\n","\n","updater = training.StandardUpdater(train_iter, optimizer, device=device)\n","trainer = training.Trainer(updater, (100, 'epoch'), out='result')\n","\n","trainer.extend(extensions.Evaluator(validation_iter, model, device=device))\n","trainer.extend(extensions.LogReport())\n","trainer.extend(extensions.PrintReport(['epoch', 'main/accuracy', 'validation/main/accuracy']))\n","\n","trainer.run()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xA6D-tIAZ5UC","colab_type":"text"},"cell_type":"markdown","source":["## 評価"]},{"metadata":{"id":"16wRi6-XZ5UD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import json\n","\n","with open('result/log') as f:\n","    logs = json.load(f)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i0cuLyWlZ5UF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["loss_train = [log['main/loss'] for log in logs]\n","loss_validation = [log['validation/main/loss'] for log in logs]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rzbVqJobZ5UH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["plt.plot(loss_train, label='loss_train')\n","plt.plot(loss_validation, label='loss_validation')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jwY1MF86Z5UI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["test = PreprocessedDataset(test, random=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SMkHsDmLZ5UK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["test_iter = chainer.iterators.SerialIterator(test, batch_size=100, repeat=False, shuffle=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VW0OKAq4Z5UM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from chainer.dataset import concat_examples\n","\n","test_accuracies = []\n","\n","while True:\n","    test_batch = test_iter.next()\n","    x_test, t_test = concat_examples(test_batch, device)\n","    \n","    pred_test = net(x_test)\n","    accuracy = F.accuracy(pred_test, t_test)\n","    accuracy.to_cpu()\n","    test_accuracies.append(accuracy.data)\n","    \n","    if test_iter.is_new_epoch:\n","        test_iter.epoch = 0\n","        test_iter.current_position = 0\n","        test_iter.is_new_epoch = False\n","        test_iter._pushed_position = None\n","        break\n","\n","print('Accuracy: {}'.format(np.mean(test_accuracies)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LIcUrtqZZ5UO","colab_type":"text"},"cell_type":"markdown","source":["## 保存"]},{"metadata":{"id":"d0p_H4EnZ5UP","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["serializers.save_npz('my.model', model)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9H9lzDHMZ5UR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}